<!DOCTYPE html>
<html>
<head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta charset="utf-8">
  <meta name="description"
        content="we bring a novel perspective to this challenge, connecting this classic problem to recent progresses in generative models.">
  <meta name="keywords" content="Geometry processing, Langevin dynamics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://richard-guyunqi.github.io/">Yunqi Gu</a><sup>1</sup>,
            </span>
              <a href="https://ianhuang.ai/">Ian Huang</a><sup>1</sup>,
            </span>     
              <a href="https://jihyeonje.com/">Jihyeon Je</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.guandaoyang.com/">Guandao Yang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Stanford University</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/images/siga24_sym_arxiv.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.02786"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://colab.research.google.com/drive/1mzytIuqjgIj2D_K3VTt-qhMtluVdVBGg?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/tea.png" alt="Teaser Image" style="width:100%;">
      <h2 class="subtitle has-text-centered" style="font-size: 18px;"> 
        BlenderGym, a 3D graphics benchmark that tasks VLMs with 3D scene reconstruction through code editing. BlenderGym consists of 245 handcrafted start-goal scene pairs across five key graphics editing tasks: object placement, lighting adjustment, procedural material editing, blend shape manipulation, and procedural geometry editing.      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          3D graphics editing is a crucial component in applications like movie production and game design, yet it remains a time-consuming process that demands highly specialized domain expertise. Automating the process is challenging because graphical editing requires performing a variety of tasks, each requiring distinct skill sets. Recently, vision-language models (VLMs) have emerged as a powerful framework for automating the editing process, but their development and evaluation are bottlenecked by the lack of a comprehensive benchmark that requires human-level perception and presents real-world editing complexity. In this work, we present \(\textbf{BlenderGym}\) , a comprehensive VLM system benchmark for 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D reconstruction tasks. We evaluate closed- and open-source VLM systems and observe that even the state-of-the-art VLM system struggles with tasks relatively easy for human Blender users. Enabled by BlenderGym, we study how inference scaling techniques impact VLM's performance on graphics editing tasks. Notably, our findings reveal that the verifier used to guide the scaling of generation can itself be improved through inference scaling, complementing recent insights on inference scaling of LLM generation in coding and math tasks. We further show that inference compute is not uniformly effective and can be optimized by strategically distributing it between generation and verification. 
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <hr>
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <h2 class="title is-3">Overview</h2>  
            <div class="hero-body">
              <img src="./static/images/overview.png" alt="Overview Image" style="width:100%;">
              <h2 class="subtitle has-text-centered" style="font-size: 16px;"> 
                Examples of task instances and VLM system output edits. We present start scene, goal scene, human user edit, and VLM system edits side by side with their corresponding metric values.
            </div>  
            <div class="content has-text-justified" >
              We aim to enable comprehensive comparison and analysis of VLM systems on 3D graphics editing tasks, which is currently bottlenecked by (1) the incomprehensive coverage of task settings with current evaluation approaches and (2) the absence of robust metrics due to unscalability of human evaluation and unreliability of AI-judge. 
              To overcome these two challenges, we introduce BlenderGymâ€”the first VLM systems benchmark tailored to 3D graphics. BlenderGym features in (1) the comprehensive coverage of essential graphics editing tasks with support for a wide range of VLM systems, (2) quantitative evaluation with image and 3D metrics, eliminating the need for human or AI-based evaluations and (3) support for inference time scaling experiments on graphics editing tasks.
              BlenderGym consists of 245 hand-crafted Blender scenes across 5 key graphics editing tasks: procedural geometry editing, lighting adjustments, procedural material design, blend shape manipulation, and object placement (45/30/30/65/30 instances respectively). Each instance in BlenderGym presents a reconstruction task from a start scene to a goal scene, where the start scene serves as the original input for the VLM system, and the goal scene represents the desired output state that the VLM system is tasked with reconstructing. Each start-goal instance includes a base Blender file of the scene setup, a pair of Python scripts that generate the start and goal scene, rendered images for both scenes, and language description of the differences between the two scenes. 
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
    <hr>
        <section class="section">
          <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-full">
                <h2 class="title is-3">LeaderBoard</h2>
              </div>
            </div>
            
            <div style="display: flex; justify-content: center; padding: 20px; background-color: #f8f8f8;">
              <div style="overflow-x: auto;">
                <table class="js-sort-table" id="results" style="width: 100%; border-collapse: collapse; text-align: center;">
                    <tr>
                      <td rowspan="2"><strong>#</strong></td>
                      <td rowspan="2"><strong>Model</strong></td>
                      <td rowspan="2"><strong>Source</strong></td>
                      <td rowspan="2"><strong>Date</strong></td>
                      <td colspan="3"><strong>Blend Shape</strong></td>
                      <td colspan="3"><strong>Placement</strong></td>
                      <td colspan="3"><strong>Geometry</strong></td>
                      <td colspan="2"><strong>Lighting</strong></td>
                      <td colspan="2"><strong>Material</strong></td>
                  </tr>
                  <tr>
                      <!-- Blend Shape -->
                      <td><strong>PL</strong></td>
                      <td><strong>N-CLIP</strong></td>
                      <td><strong>CD</strong></td>
                  
                      <!-- Placement -->
                      <td><strong>PL</strong></td>
                      <td><strong>N-CLIP</strong></td>
                      <td><strong>CD</strong></td>
                  
                      <!-- Geometry -->
                      <td><strong>PL</strong></td>
                      <td><strong>N-CLIP</strong></td>
                      <td><strong>CD</strong></td>
                  
                      <!-- Lighting -->
                      <td><strong>PL</strong></td>
                      <td><strong>N-CLIP</strong></td>
                  
                      <!-- Material -->
                      <td><strong>PL</strong></td>
                      <td><strong>N-CLIP</strong></td>
                  </tr>                  
                    <tr>
                      <td>1</td>
                      <td style="white-space: nowrap;"><b class="best-score-text">InternVL2-8B adfgadfasgd ðŸ¥‡</b></td>
                      <td><a href="https://github.com/OpenGVLab/InternVL" class="ext-link" style="font-size: 16px;">Link</a></td>
                      <td>2024-09-04</td>
                      <td><b class="best-score-text">65.84</b></td>
                      <td>65.0</td>
                      <td>64.0</td>
                      <td>75.4</td>
                      <td>72.4</td>
                      <td>52.3</td>
                      <td>67.4</td>
                      <td>63.1</td>
                      <td>65.0</td>
                      <td>30.4</td>
                      <td>44.5</td>
                      <td>67.0</td>
                      <td>72.5</td>                                
                    </tr>
                    <tr>
                      <td>2</td>
                      <td style="white-space: nowrap;"><b class="best-score-text">InternVL2-8B-MPO asgfgafdhs ðŸ¥ˆ</b></td>
                      <td><a href="https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/" class="ext-link" style="font-size: 16px;">Link</a></td>
                      <td>2024-11-14</td>
                      <td><b class="best-score-text">65.65</b></td>
                      <td>67.4</td> <!-- FQA (figure question answering) -->
                      <td>68.0</td> <!-- GPS (geometry problem solving) -->
                      <td>73.0</td> <!-- MWP (math word problem) -->
                      <td>65.3</td> <!-- TQA (textbook question answering) -->
                      <td>51.5</td> <!-- VQA (visual question answering) -->
                      <td>66.7/td> <!-- ALG (algebraic reasoning) -->
                      <td>60.6</td> <!-- ARI (arithmetic reasoning) -->
                      <td>68.5</td> <!-- GEO (geometry reasoning) -->
                      <td>19.1</td> <!-- LOG (logical reasoning) -->
                      <td>43.1</td> <!-- NUM (numeric commonsense) -->
                      <td>64.9</td> <!-- SCI (scientific reasoning) -->
                      <td>77.1</td> <!-- STA (statistical reasoning) -->                                
                    </tr>
                                                  
                </table>
                  <br>
                  <div>
                  <p>ðŸš¨ To submit your results to the leaderboard, please send to <a href="mailto:lupantech@gmail.com">this email</a> with your result json files.</p>
                  <p>ðŸš¨ For more submission details, please refer to <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-leaderboard-">this link</a> and <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-evaluations-on-mathvista">this link</a>.
                  </p>
                  </div>
                </div>
              </div>
            </div>
            
            <br>
          </section>
    <hr>
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <h2 class="title is-3">Verifier Scaling</h2>
            <div class="content has-text-justified">
              To complement recent findings on inference scaling of VLM/LLM generation, we explore improving the verifier that guides the generation by selecting desirable edits and pruning suboptimal ones. 
              We find that similar to generation, VLM verifiers used for guiding generation also benefit from inference scaling, and that scaled open-source VLM verifiers can exceed the performance of closed-source VLM verifiers.
              With that knowledge, we further explore the question of distributing inference compute between generation and verification. We find that (1) the distribution of inference compute significantly impacts performance, and that (2) the optimal compute ratio between generation and verification varies with the amount of total compute â€” more total compute benefits from a higher share of verification.
              </div>
          </div>
        </div>
        <br>
      </section>
<hr>
<section class="section no-spacing">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-6">Acknowledgements</h2>
        <div class="content has-text-justified">
            We acknowledge the support of ARL grant W911NF-21-2-0104 and a Vannevar Bush Faculty Fellowship. Additionally, We would like to thank Yangjun Ruan, Haiwen (Haven) Feng, Jordan Juravsky, and all our reviewers for feedback on paper revisions.    </div>
  </section>
  <section class="section no-spacing" id="Bibtex">
    <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
Our citation address here.
    </code></pre>
  </div>
</section>


<footer class="footer" style="padding: 10px 0;">

  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content" style="font-size: 0.875rem; line-height: 1.25; margin: 0;">
        <p style="margin-bottom: 5px;">
          This website is licensed under a <a rel="license"
                                              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
        <p style="margin-bottom: 0;">
          The website is based on <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We thank the original authors for open-sourcing their code. 
        </p>
      </div>
    </div>
  </div>

</footer>


</body>
</html>
