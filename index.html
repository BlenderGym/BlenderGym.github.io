<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating mathematical reasoning of foundation models in visual contexts">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/mathvista.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>

  <script>
    // Keep track of sort directions by column index
    const sortDirections = {};
  
    /**
     * Sort the table by the given column index.
     * @param {string} tableId    - The ID of the table (e.g. "results").
     * @param {number} colIndex   - Zero-based index of the column to sort by.
     * @param {Element} el        - The span element that was clicked.
     */
    function sortTableByColumn(tableId, colIndex, el) {
      const table = document.getElementById(tableId);
      if (!table) return;
  
      // Convert NodeList to Array
      const allRows = Array.from(table.querySelectorAll("tr"));
      
      // We have 2 header rows in this example:
      const headerRows = allRows.slice(0, 2); // first 2 = header
      const bodyRows   = allRows.slice(2);    // rest = data
  
      // Toggle the sort direction for this column
      const currentDir = sortDirections[colIndex] || "asc";
      const nextDir    = currentDir === "asc" ? "desc" : "asc";
      sortDirections[colIndex] = nextDir;
  
      // Sort the body rows
      bodyRows.sort((a, b) => {
        const cellA = a.getElementsByTagName("td")[colIndex];
        const cellB = b.getElementsByTagName("td")[colIndex];
  
        // For empty or "--" cells, treat them as 0
        const valA = parseFloat(cellA?.textContent.replace(/--/, "1000")) || 0;
        const valB = parseFloat(cellB?.textContent.replace(/--/, "1000")) || 0;
  
        if (nextDir === "asc") {
          return valA - valB;
        } else {
          return valB - valA;
        }
      });
  
      // Re-attach sorted rows
      const tbody = table.tBodies[0];
      bodyRows.forEach(row => tbody.appendChild(row));
  
      // Now update the clicked column‚Äôs label text with an up/down arrow
      //  - up arrow can be "‚ñ¥" or "‚ñ≤"
      //  - down arrow can be "‚ñæ" or "‚ñº"
      const label = el.dataset.colLabel || "";
      const arrow = nextDir === "asc" ? "‚ñ¥" : "‚ñæ";
      el.textContent = label + " " + arrow;  // e.g. "PL ‚ñ¥" or "PL ‚ñæ"
  
      // For all other columns in the header, revert to plain label (no arrow)
      headerRows.forEach((hdr) => {
        hdr.querySelectorAll(".sort-toggle").forEach(span => {
          if (span !== el) {
            // remove arrow from others
            const colLabel = span.dataset.colLabel || "";
            span.textContent = colLabel;
          }
        });
      });
    }
  </script>
  

<style>
  .sort-toggle {
  cursor: pointer;
  user-select: none;
  text-decoration: none;
  border: none;
  background: none;
  padding: 0;
  margin: 0;
  font: inherit;
}
</style>
  
  
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-1 publication-title is-bold">
            <!-- <img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/> -->
            <span class="mathvista" style="vertical-align: middle">BlenderGym</span>
          </h2>
          <h2 style="font-size: 22px;">
            Benchmarking Foundational Model Systems for Graphics Editing
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://richard-guyunqi.github.io/">Yunqi Gu</a>,
            </span>
              <a href="https://ianhuang.ai/">Ian Huang</a>,
            </span>     
              <a href="https://jihyeonje.com/">Jihyeon Je</a>,
            </span>
            <span class="author-block">
              <a href="https://www.guandaoyang.com/">Guandao Yang</a>,
            </span>
            <span class="author-block">
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Stanford University</span><br>
            <span class="paper-block"><b>CVPR 2025</b></span>
          </div>
        
          <!-- <section> -->
            <!-- <div class="section" id="org-banners" style="display:fle">
              <a href="https://www.ucla.edu/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/ucla.png" style="height:3em">
              </a>
              <a href="https://www.washington.edu/" target="blank" class="ext-link">
                  <img class="center-block org-banner" src="static/images/uw.png" style="height:3em">
              </a>
              <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/microsoft.png" style="height:3em">
              </a>
            </div> -->
          <!-- </section> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- @PAN TODO: change links -->
                <a href="https://arxiv.org/pdf/2310.02255.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2310.02255"
                   class="external-link button is-normal is-rounded is-dark">
                <!-- <a href="https://lupantech.github.io/papers/arxiv23_mathvista.pdf"
                   class="external-link button is-normal is-rounded is-dark"> -->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lupantech/MathVista"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/AI4Math/MathVista"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ü§ó</p>
                      <!-- üîó -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="https://blendergym.github.io/#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üèÜ</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
              <!-- Twitter Link. -->
              <span class="link-block">
                <a href="https://twitter.com/lupantech/status/1717313355780964608"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <!-- üíªüîó -->
                      <p style="font-size:18px">üåê</p>
                  </span>
                  <span>Twitter</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="static/images/tease_scores_gpt4v.png" alt="geometric reasoning" width="99%"/>
      <p> Accuracy scores of one leading LLM (i.e., PoT GPT-4), four primary LMMs, random chance, and human performance our proposed 
      <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
      <span class="mathvista">MathVista</span>
      across mathematical reasoning and visual context types. PoT refers to program-of-thought prompting, and PoT GPT-4 is a textual LLM augmented with the caption and OCR text. GPT-4V is manually evaluated via the playground chatbot.
      </p>
    </div>
  </div>
</section> -->

<section>
  <div style="background: linear-gradient(to right, #9bebf2, #f7cc86);
              padding: 1rem;
              border-radius: 5px;
              margin-bottom: 1em;
              text-align: center;">
    <p style="font-weight: bold;
              font-size: 22px;
              width: 80%;
              margin: 0 auto;
              text-align: center;">
      TLDR: We present <strong>BlenderGym</strong>, the first VLM system benchmark for 3D graphics that tasks VLMs with code-based 3D scene reconstruction.
    </p>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure>
        <img src="./static/images/tea.png" alt="Teaser Image" style="width: 100%; display: block; margin: 0 auto;">
        <figcaption class="subtitle" 
                    style="font-size: 18px; width: 80%; text-align: center; margin: 1rem auto 0 auto; display: block;">
          BlenderGym consists of 245 handcrafted start-goal scene pairs across five key graphics editing tasks:
          object placement, lighting adjustment, procedural material editing, blend shape manipulation, and 
          procedural geometry editing.
        </figcaption>
      </figure>
    </div>
  </div>
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D graphics editing is a crucial component in applications like movie production and game design, yet it remains a time-consuming process that demands highly specialized domain expertise. Automating the process is challenging because graphical editing requires performing a variety of tasks, each requiring distinct skill sets. Recently, vision-language models (VLMs) have emerged as a powerful framework for automating the editing process, but their development and evaluation are bottlenecked by the lack of a comprehensive benchmark that requires human-level perception and presents real-world editing complexity. 
          </p>
          <p>
            In this work, we present <b>BlenderGym</b>, a comprehensive VLM system benchmark for 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D reconstruction tasks. We evaluate closed- and open-source VLM systems and observe that even the state-of-the-art VLM system struggles with tasks relatively easy for human Blender users. 
          </p>
          <p>
            Enabled by BlenderGym, we study how <b>inference scaling on verification</b> impacts VLM's performance on graphics editing tasks. Notably, our findings reveal that the verifier used to guide the scaling of generation can itself be improved through inference scaling, complementing recent insights on inference scaling of LLM generation in coding and math tasks. We further show that inference compute is not uniformly effective and can be optimized by strategically distributing it between generation and verification. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="hero-body">
          <img src="./static/images/overview.png" alt="Overview Image" style="width:90%;">
          <h2 class="subtitle has-text-centered" style="font-size: 16px;"> 
            Examples of task instances and VLM system output edits. We present start scene, goal scene, human user edit, and VLM system edits side by side with their corresponding metric values.
        </div>  
        <div class="content has-text-justified">
          <p>
            We aim to enable comprehensive comparison and analysis of VLM systems on 3D graphics editing tasks, which is currently bottlenecked by (1) the incomprehensive coverage of task settings with current evaluation approaches and (2) the absence of robust metrics due to unscalability of human evaluation and unreliability of AI-judge. 
          </p>
          <p>
            To overcome these two challenges, we introduce <b>BlenderGym</b>‚Äîthe first VLM system benchmark tailored to 3D graphics. BlenderGym features in (1) the comprehensive coverage of essential graphics editing tasks with support for a wide range of VLM systems, (2) quantitative evaluation with image and 3D metrics, eliminating the need for human or AI-based evaluations and (3) support for inference time scaling experiments on graphics editing tasks.
          </p>
          <p>
            BlenderGym consists of 245 hand-crafted Blender scenes across 5 key graphics editing tasks: procedural geometry editing, lighting adjustments, procedural material design, blend shape manipulation, and object placement (45/30/30/65/30 instances respectively). Each instance in BlenderGym presents a reconstruction task from a start scene to a goal scene, where the start scene serves as the original input for the VLM system, and the goal scene represents the desired output state that the VLM system is tasked with reconstructing. Each start-goal instance includes a base Blender file of the scene setup, a pair of Python scripts that generate the start and goal scene, rendered images for both scenes, and language description of the differences between the two scenes. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">

        <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
        <div class="content">

        <table class="js-sort-table" id="results">
          <tr>
              <td rowspan="2"><strong>#</strong></td>
              <td rowspan="2"><strong>Model</strong></td>
              <td rowspan="2"><strong>Date</strong></td>
              <td colspan="3"><strong>Blend Shape</strong></td>
              <td colspan="3"><strong>Placement</strong></td>
              <td colspan="3"><strong>Geometry</strong></td>
              <td colspan="2"><strong>Lighting</strong></td>
              <td colspan="2"><strong>Material</strong></td>
          </tr>
          <tr>
            <!-- Blend Shape: PL (column 3), N-CLIP (4), CD (5) -->
            <td>
              <span 
                class="sort-toggle" 
                data-col-label="PL"
                onclick="sortTableByColumn('results', 3, this)"
              >
                PL
              </span>
            </td>
            <td>
              <span
                class="sort-toggle"
                data-col-label="N-CLIP"
                onclick="sortTableByColumn('results', 4, this)"
              >
                N-CLIP
              </span>
            </td>
            <td>
              <span
                class="sort-toggle"
                data-col-label="CD"
                onclick="sortTableByColumn('results', 5, this)"
              >
                CD
              </span>
            </td>
          
            <!-- Placement: PL (6), N-CLIP (7), CD (8) -->
            <td>
              <span 
                class="sort-toggle" 
                data-col-label="PL"
                onclick="sortTableByColumn('results', 6, this)"
              >
                PL
              </span>
            </td>
            <td>
              <span
                class="sort-toggle"
                data-col-label="N-CLIP"
                onclick="sortTableByColumn('results', 7, this)"
              >
                N-CLIP
              </span>
            </td>
            <td>
              <span
                class="sort-toggle"
                data-col-label="CD"
                onclick="sortTableByColumn('results', 8, this)"
              >
                CD
              </span>
            </td>
          
            <!-- Geometry: PL (9), N-CLIP (10), CD (11) -->
            <td>
              <span
                class="sort-toggle"
                data-col-label="PL"
                onclick="sortTableByColumn('results', 9, this)"
              >
                PL
              </span>
            </td>
            <td>
              <span
                class="sort-toggle"
                data-col-label="N-CLIP"
                onclick="sortTableByColumn('results', 10, this)"
              >
                N-CLIP
              </span>
            </td>
            <td>
              <span
                class="sort-toggle"
                data-col-label="CD"
                onclick="sortTableByColumn('results', 11, this)"
              >
                CD
              </span>
            </td>
          
            <!-- Lighting: PL (12), N-CLIP (13) -->
            <td>
              <span
                class="sort-toggle"
                data-col-label="PL"
                onclick="sortTableByColumn('results', 12, this)"
              >
                PL
              </span>
            </td>
            <td>
              <span
                class="sort-toggle"
                data-col-label="N-CLIP"
                onclick="sortTableByColumn('results', 13, this)"
              >
                N-CLIP
              </span>
            </td>
          
            <!-- Material: PL (14), N-CLIP (15) -->
            <td>
              <span
                class="sort-toggle"
                data-col-label="PL"
                onclick="sortTableByColumn('results', 14, this)"
              >
                PL
              </span>
            </td>
            <td>
              <span
                class="sort-toggle"
                data-col-label="N-CLIP"
                onclick="sortTableByColumn('results', 15, this)"
              >
                N-CLIP
              </span>
            </td>
          </tr>
          
      
          <!-- 1. GPT-4o -->
          <tr>
              <td>1</td>
              <td><a href="https://platform.openai.com/docs/models/gpt-4o" target="_blank">GPT-4o</a></td>
              <td>2024-08-06</td>
              <!-- Blend Shape -->
              <td><b>9.140</b></td>
              <td><b>20.47</b></td>
              <td><b>0.904</b></td>
              <!-- Placement -->
              <td>11.89</td>
              <td><b>30.38</b></td>
              <td>11.22</td>
              <!-- Geometry -->
              <td><b>6.747</b></td>
              <td>8.561</td>
              <td>1.192</td>
              <!-- Lighting -->
              <td><b>2.410</b></td>
              <td>2.398</td>
              <!-- Material -->
              <td><b>3.653</b></td>
              <td>8.942</td>
          </tr>
      
          <!-- 2. Claude-3.5-Sonnet -->
          <tr>
              <td>2</td>
              <td><a href="https://www.anthropic.com/news/claude-3-5-sonnet" target="_blank">Claude-3.5-Sonnet</a></td>
              <td>2024-10-22</td>
              <!-- Blend Shape -->
              <td>12.79</td>
              <td>27.96</td>
              <td>1.962</td>
              <!-- Placement -->
              <td>13.19</td>
              <td>51.76</td>
              <td>11.29</td>
              <!-- Geometry -->
              <td>10.81</td>
              <td>13.04</td>
              <td>1.452</td>
              <!-- Lighting -->
              <td>2.897</td>
              <td>4.049</td>
              <!-- Material -->
              <td>5.769</td>
              <td>11.44</td>
          </tr>
      
          <!-- 3. GPT-4-Turbo -->
          <tr>
              <td>3</td>
              <td><a href="https://platform.openai.com/docs/models/gpt-4-turbo" target="_blank">GPT-4-Turbo</a></td>
              <td>2024-04-09</td>
              <!-- Blend Shape -->
              <td>15.21</td>
              <td>26.15</td>
              <td>1.927</td>
              <!-- Placement -->
              <td>12.21</td>
              <td>37.57</td>
              <td>12.80</td>
              <!-- Geometry -->
              <td>8.160</td>
              <td>10.92</td>
              <td><b>1.120</b></td>
              <!-- Lighting -->
              <td>2.723</td>
              <td>3.912</td>
              <!-- Material -->
              <td>5.424</td>
              <td><b>8.812</b></td>
          </tr>
      
          <!-- 4. Claude-3-Haiku -->
          <tr>
              <td>4</td>
              <td><a href="https://www.anthropic.com/news/claude-3-haiku" target="_blank">Claude-3-Haiku</a></td>
              <td>2024-03-07</td>
              <!-- Blend Shape -->
              <td>13.62</td>
              <td>29.72</td>
              <td>2.563</td>
              <!-- Placement -->
              <td>14.78</td>
              <td>44.10</td>
              <td>12.13</td>
              <!-- Geometry -->
              <td>10.15</td>
              <td>12.51</td>
              <td>1.362</td>
              <!-- Lighting -->
              <td>3.712</td>
              <td>4.824</td>
              <!-- Material -->
              <td>5.960</td>
              <td>11.61</td>
          </tr>
      
          <!-- 5. Gemini-1.5-flash -->
          <tr>
              <td>5</td>
              <td><a href="https://ai.google.dev/gemini-api/docs/models#gemini-1.5-flash" target="_blank">Gemini-1.5-flash</a></td>
              <td>2024-9</td>
              <!-- Blend Shape -->
              <td>23.18</td>
              <td>30.47</td>
              <td>2.412</td>
              <!-- Placement -->
              <td><b>10.94</b></td>
              <td>45.34</td>
              <td><b>8.324</b></td>
              <!-- Geometry -->
              <td>9.443</td>
              <td>10.49</td>
              <td>1.323</td>
              <!-- Lighting -->
              <td>3.514</td>
              <td>5.688</td>
              <!-- Material -->
              <td>6.364</td>
              <td>10.42</td>
          </tr>
      
          <!-- 6. Qwen2-vl-7b -->
          <tr>
              <td>6</td>
              <td><a href="https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct-AWQ" target="_blank">Qwen2-VL-7b-Instruct</a></td>
              <td>2024-09-25</td>
              <!-- Blend Shape -->
              <td>16.78</td>
              <td>29.22</td>
              <td>2.123</td>
              <!-- Placement -->
              <td>15.31</td>
              <td>41.12</td>
              <td>14.21</td>
              <!-- Geometry -->
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <!-- Lighting -->
              <td>2.985</td>
              <td><b>2.225</b></td>
              <!-- Material -->
              <td>--</td>
              <td>--</td>
          </tr>
      
          <!-- 7. Qwen-Llama -->
          <tr>
              <td>7</td>
              <td>Qwen-Llama</td>
              <td>N/A</td>
              <!-- Blend Shape -->
              <td>14.32</td>
              <td>28.23</td>
              <td>2.012</td>
              <!-- Placement -->
              <td>14.65</td>
              <td>34.93</td>
              <td>12.41</td>
              <!-- Geometry -->
              <td>13.97</td>
              <td>14.13</td>
              <td>1.673</td>
              <!-- Lighting -->
              <td>3.173</td>
              <td>3.998</td>
              <!-- Material -->
              <td>--</td>
              <td>--</td>
          </tr>
      
          <!-- 8. Phi-3.5-vision -->
          <tr>
              <td>8</td>
              <td><a href="https://huggingface.co/microsoft/Phi-3.5-vision-instruct" target="_blank">Phi-3.5-vision</a></td>
              <td>2024-08-20</td>
              <!-- Blend Shape -->
              <td>12.51</td>
              <td>24.14</td>
              <td>2.012</td>
              <!-- Placement -->
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <!-- Geometry -->
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <!-- Lighting -->
              <td>3.127</td>
              <td>6.012</td>
              <!-- Material -->
              <td>--</td>
              <td>--</td>
          </tr>
      
          <!-- 9. Phi-Llama -->
          <tr>
              <td>9</td>
              <td>Phi-Llama</td>
              <td>N/A</td>
              <!-- Blend Shape -->
              <td>12.13</td>
              <td>24.77</td>
              <td>1.826</td>
              <!-- Placement -->
              <td>14.61</td>
              <td>35.61</td>
              <td>12.61</td>
              <!-- Geometry -->
              <td>9.818</td>
              <td>11.92</td>
              <td>1.471</td>
              <!-- Lighting -->
              <td>3.621</td>
              <td>6.895</td>
              <!-- Material -->
              <td>--</td>
              <td>--</td>
          </tr>
      
          <!-- 10. MiniCPM-V-2.6 -->
          <tr>
              <td>10</td>
              <td><a href="https://huggingface.co/openbmb/MiniCPM-V-2_6" target="_blank">MiniCPM-V-2.6</a></td>
              <td>2024-08-06</td>
              <!-- Blend Shape -->
              <td>13.86</td>
              <td>29.92</td>
              <td>1.997</td>
              <!-- Placement -->
              <td>11.99</td>
              <td>31.69</td>
              <td>12.62</td>
              <!-- Geometry -->
              <td>7.127</td>
              <td><b>8.542</b></td>
              <td>1.229</td>
              <!-- Lighting -->
              <td>3.829</td>
              <td>6.124</td>
              <!-- Material -->
              <td>--</td>
              <td>--</td>
          </tr>
      
          <!-- 11. MiniCPM-Llama -->
          <tr>
              <td>11</td>
              <td>MiniCPM-Llama</td>
              <td>N/A</td>
              <!-- Blend Shape -->
              <td>13.76</td>
              <td>27.21</td>
              <td>1.882</td>
              <!-- Placement -->
              <td>12.74</td>
              <td>31.72</td>
              <td>15.81</td>
              <!-- Geometry -->
              <td>9.561</td>
              <td>11.47</td>
              <td>1.569</td>
              <!-- Lighting -->
              <td>3.725</td>
              <td>6.090</td>
              <!-- Material -->
              <td>7.152</td>
              <td>12.14</td>
          </tr>
      
          <!-- 12. InternVL2-8b -->
          <tr>
              <td>12</td>
              <td><a href="https://huggingface.co/OpenGVLab/InternVL2-8B" target="_blank">InternVL2-8b</a></td>
              <td>2024-07-04</td>
              <!-- Blend Shape -->
              <td>12.69</td>
              <td>29.09</td>
              <td>1.920</td>
              <!-- Placement -->
              <td>14.71</td>
              <td>35.92</td>
              <td>17.22</td>
              <!-- Geometry -->
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <!-- Lighting -->
              <td>3.920</td>
              <td>6.825</td>
              <!-- Material -->
              <td>--</td>
              <td>--</td>
          </tr>
      
          <!-- 13. Intern-Llama -->
          <tr>
              <td>13</td>
              <td>Intern-Llama</td>
              <td>N/A</td>
              <!-- Blend Shape -->
              <td>11.80</td>
              <td>23.83</td>
              <td>1.861</td>
              <!-- Placement -->
              <td>16.15</td>
              <td>37.23</td>
              <td>18.22</td>
              <!-- Geometry -->
              <td>13.70</td>
              <td>14.44</td>
              <td>1.578</td>
              <!-- Lighting -->
              <td>3.825</td>
              <td>6.152</td>
              <!-- Material -->
              <td>--</td>
              <td>--</td>
          </tr>
      
          <!-- 14. Human -->
          <tr>
              <td> </td>
              <td><b>Human</b></td>
              <td>2025-1-15</td>
              <!-- Blend Shape -->
              <td><b>0.934</b></td>
              <td><b>9.12</b></td>
              <td><b>0.399</b></td>
              <!-- Placement -->
              <td><b>0.423</b></td>
              <td><b>13.34</b></td>
              <td><b>1.532</b></td>
              <!-- Geometry -->
              <td><b>1.269</b></td>
              <td><b>2.434</b></td>
              <td><b>0.334</b></td>
              <!-- Lighting -->
              <td><b>1.239</b></td>
              <td><b>1.632</b></td>
              <!-- Material -->
              <td><b>0.629</b></td>
              <td><b>3.043</b></td>
          </tr>
      </table>
          
        </div>

      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Inference Scaling for Verifier</h2>
        <div class="content has-text-justified">
          <p>
            To complement recent findings on inference scaling of VLM/LLM generation, we explore improving the verifier 
            that guides the generation by selecting desirable edits and pruning suboptimal ones.
          </p>
          <!-- First Figure (80% width, centered) -->
          <div style="text-align: center;">
            <figure style="width:50%; margin:0 auto;">
              <img src="./static/images/veri_scaling.png" 
                   alt="Verifier scaling figure"
                   style="width:100%; height:auto; object-fit:contain;">
              <figcaption style="font-size:13px; text-align:center;">
                Performance of inference scaling for VLM verifier with InternVL2-8B, Claude3.5 Sonnet, and GPT-4o.
              </figcaption>
            </figure>
          </div>
          <p>
            As shown by the figure above, VLM verifiers used for guiding generation also benefit from inference scaling, 
            and that scaled open-source VLM verifiers can exceed the performance of closed-source VLM verifiers.
          </p>
          <p>
            With that knowledge, we further explore the question of distributing inference compute between generation 
            and verification. We show with the figure below that (1) the distribution of inference compute significantly 
            impacts performance, and that (2) the optimal compute ratio between generation and verification varies with 
            the amount of total compute ‚Äî more total compute benefits from a higher share of verification.
          </p>
          <!-- Second Figure (120% width, centered) -->
          <div style="text-align: center;">
            <figure style="width:100%; margin:0 auto;">
              <img src="./static/images/allocation_all.png"
                   alt="Allocation of compute figure"
                   style="width:100%; height:auto; object-fit:contain;">
              <figcaption style="font-size:13px; text-align:center;">
                The impact of compute allocation on VLM system performance. We set VeriRatio(verification compute over total compute) to 0.33, 0.62, and 0.73. <br>
                We observe that when more compute is available, a higher share of verification yields greater benefits.
              </figcaption>
            </figure>
          </div>
          <!-- <div class="hero-body">
            <img src="./static/images/allocation_all.png" alt="Overview Image" style="width:100%;">
            <h2 class="subtitle has-text-centered" style="font-size: 16px;"> 
              The impact of compute allocation on VLM system performance. <br>
              We set VeriRatio(verification compute over total compute) to 0.33, 0.62, and 0.73. We observe that when 
              more compute is available, a higher share of verification yields greater benefits.
        </div>   -->
        </div>
      </div>
    </div>
  </div>
  <br>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Human-VLM Verifier Alignment</h2>
        <div class="content has-text-justified">
          <p>
            We found that the verifier sometimes fails to distinguish between desirable and suboptimal edits. Certain VLMs, such as Qwen2-vl-7b, consistently favor the second edit candidate in the pair, regardless of how we permute them.
          </p>

          <p>
            We measure human-VLM verifier alignment to ground our verifier analysis more quantitatively. For each of the five tasks, we select one instance and collect all the pair-wise selection decisions that the VLM verifier made throughout the inference. We collect 7,950 pair-wise judgments from 50 participants. For each pair-wise selection, VLM and human verifiers are asked to choose the edit render closest to the goal render. We compute the alignment rate between two verifiers by calculating the ratio of the number of aligned pair-wise selections over the total number of pairs.
          </p>

          <div class="hero-body">
            <img src="./static/images/verifier_alignment_.jpg" 
                 alt="Verifier alignment Image" 
                 style="display: block; margin: 0 auto; width: 65%;">
            <figcaption class="subtitle has-text-centered" style="font-size: 16px;"> 
              Human-VLM and inter-human verifier alignment rate. All models perform above the random baseline (0.5) yet differ notably, with even the highest-aligned model, Claude-3.5-Sonnet (0.66), falling short of inter-human alignment (0.79).
            </figcaption>
          </div>          
          <p>
            Results above reveal that human verifiers achieve a significantly higher alignment rate compared to VLMs, highlighting considerable room for improvement in VLM verifier performance.          </p>
        </div>
    </div>
  </div>
  <br>
</section>


<section class="section no-spacing">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-6">Acknowledgements</h2>
        <div class="content has-text-justified">
            We acknowledge the support of ARL grant W911NF-21-2-0104 and a Vannevar Bush Faculty Fellowship. Additionally, We would like to thank Yangjun Ruan, Haiwen (Haven) Feng, Jordan Juravsky, and all our reviewers for feedback on paper revisions.    </div>
</section>

<section class="section no-spacing" id="Bibtex">
    <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
Our citation address here.
    </code></pre>
  </div>
</section>


<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>